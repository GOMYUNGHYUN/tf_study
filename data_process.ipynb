{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('data/fake_or_real_news.csv',usecols=[1,2,3])\n",
    "\n",
    "# 문장 분리\n",
    "df.title=df.title.map(lambda x: [x])\n",
    "df.text=df.text.map(lambda x: [i.strip() for i in x.split('\\n')])\n",
    "df['data']=df.title+df.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[You Can Smell Hillary’s Fear]</td>\n",
       "      <td>[Daniel Greenfield, a Shillman Journalism Fell...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>[You Can Smell Hillary’s Fear, Daniel Greenfie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Watch The Exact Moment Paul Ryan Committed Po...</td>\n",
       "      <td>[Google Pinterest Digg Linkedin Reddit Stumble...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>[Watch The Exact Moment Paul Ryan Committed Po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Kerry to go to Paris in gesture of sympathy]</td>\n",
       "      <td>[U.S. Secretary of State John F. Kerry said Mo...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>[Kerry to go to Paris in gesture of sympathy, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Bernie supporters on Twitter erupt in anger a...</td>\n",
       "      <td>[— Kaydee King (@KaydeeKing) November 9, 2016 ...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>[Bernie supporters on Twitter erupt in anger a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[The Battle of New York: Why This Primary Matt...</td>\n",
       "      <td>[It's primary day in New York and front-runner...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>[The Battle of New York: Why This Primary Matt...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                     [You Can Smell Hillary’s Fear]   \n",
       "1  [Watch The Exact Moment Paul Ryan Committed Po...   \n",
       "2      [Kerry to go to Paris in gesture of sympathy]   \n",
       "3  [Bernie supporters on Twitter erupt in anger a...   \n",
       "4  [The Battle of New York: Why This Primary Matt...   \n",
       "\n",
       "                                                text label  \\\n",
       "0  [Daniel Greenfield, a Shillman Journalism Fell...  FAKE   \n",
       "1  [Google Pinterest Digg Linkedin Reddit Stumble...  FAKE   \n",
       "2  [U.S. Secretary of State John F. Kerry said Mo...  REAL   \n",
       "3  [— Kaydee King (@KaydeeKing) November 9, 2016 ...  FAKE   \n",
       "4  [It's primary day in New York and front-runner...  REAL   \n",
       "\n",
       "                                                data  \n",
       "0  [You Can Smell Hillary’s Fear, Daniel Greenfie...  \n",
       "1  [Watch The Exact Moment Paul Ryan Committed Po...  \n",
       "2  [Kerry to go to Paris in gesture of sympathy, ...  \n",
       "3  [Bernie supporters on Twitter erupt in anger a...  \n",
       "4  [The Battle of New York: Why This Primary Matt...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGlove(dim):\n",
    "    # https://nlp.stanford.edu/projects/glove/\n",
    "    # 6B tokens, 400K voca -> 60억 토큰 사용 & 40만 단어 사전\n",
    "    df = pd.read_csv('data/glove.6B/glove.6B.%sd.txt' %(dim), sep=\" \", quoting=3, header=None, index_col=0)\n",
    "    glove = {key: val.values for key, val in df.T.items()}\n",
    "    print(\"- done. {} tokens\".format(len(list(glove.keys())))   )\n",
    "    # format -> word : vector\n",
    "    return glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- done. 399998 tokens\n"
     ]
    }
   ],
   "source": [
    "raw_glove=getGlove(glove_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict, 399998)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(raw_glove), len(raw_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(nan, array([ 0.17854 ,  0.6914  ,  0.13973 , -0.069674, -0.3969  ,  0.02561 ,\n",
       "         0.34453 ,  0.41245 ,  0.53813 , -0.1873  ,  0.69401 , -0.2321  ,\n",
       "        -0.17245 , -0.090935,  0.5075  ,  0.06169 , -0.53494 ,  0.59271 ,\n",
       "         0.10355 ,  0.19821 ,  0.080418, -0.33788 , -0.5333  , -0.19901 ,\n",
       "        -0.078666,  0.18881 , -0.33156 ,  0.14503 ,  0.23971 , -0.21635 ,\n",
       "         0.20574 ,  0.42454 , -0.41191 , -0.39644 ,  0.23624 , -0.26326 ,\n",
       "        -0.29633 , -0.22775 , -0.36422 ,  0.064039,  0.43746 ,  0.44278 ,\n",
       "        -0.026173, -0.059415,  0.15829 , -0.87434 , -0.21649 ,  0.4958  ,\n",
       "        -0.77992 ,  0.36909 ,  0.25808 ,  0.20311 ,  0.010877, -0.47275 ,\n",
       "        -0.75945 ,  0.82907 ,  0.59164 , -0.1137  , -0.43274 , -0.67147 ,\n",
       "        -0.43796 , -0.33914 , -0.83799 , -0.58472 ,  0.64412 , -0.12344 ,\n",
       "         0.13492 , -0.60226 , -0.34824 ,  0.48464 ,  0.60218 ,  0.29932 ,\n",
       "         0.14631 , -0.15356 ,  0.24187 , -0.24425 ,  0.34556 ,  0.31079 ,\n",
       "         0.30814 , -1.0065  , -0.068062, -0.61547 , -0.34414 ,  0.040753,\n",
       "         0.14102 ,  0.17741 ,  0.45974 , -0.25586 ,  0.28271 ,  0.73234 ,\n",
       "         0.052544, -0.29785 ,  0.53109 , -0.13755 ,  1.0487  , -0.66433 ,\n",
       "         0.63311 , -0.095053, -0.26814 , -0.73228 ]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(raw_glove.keys())[0], raw_glove[list(raw_glove.keys())[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model에 사용하는 데이터 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vocab(filename):\n",
    "    d = dict()\n",
    "    with open(filename, encoding='utf-8') as f:\n",
    "        for idx, word in enumerate(f):\n",
    "            word = word.strip()\n",
    "            d[word] = idx\n",
    "    # format -> dict[word] = mat index\n",
    "    return d\n",
    "def load_embedding_mat(filename):\n",
    "    with np.load(filename) as data:\n",
    "        return data[\"embeddings\"]\n",
    "\n",
    "def importProcessedData():\n",
    "    global vocabs_path\n",
    "    global glove_mat_path\n",
    "    global vocabs_path\n",
    "    global tf_idf_mat_path\n",
    "    \n",
    "    vocabs=load_vocab(vocabs_path)\n",
    "    #  [ num vocabs x glove_dim ] mat\n",
    "    glove_mat=load_embedding_mat(glove_mat_path)\n",
    "    \n",
    "    tf_idf_vocabs=load_vocab(vocabs_path)\n",
    "    #  [ num all text x tf_idf_vocabs ] mat\n",
    "    tf_idf_mat=load_embedding_mat(tf_idf_mat_path)\n",
    "    \n",
    "    return vocabs, glove_mat, tf_idf_vocabs, tf_idf_mat\n",
    "\n",
    "def importTextData():\n",
    "    global train_text_path\n",
    "    global test_text_path\n",
    "\n",
    "    train=pd.read_pickle(train_text_path)\n",
    "    test=pd.read_pickle(test_text_path)\n",
    "    \n",
    "    return train,test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabs_path='data/processed_data/vocab.txt'\n",
    "glove_mat_path='data/processed_data/embedding_mat.npz'\n",
    "tf_idf_vocabs_path='data/processed_data/tf_idf_vocab.txt'\n",
    "tf_idf_mat_path='data/processed_data/tf_idf_mat.npz'\n",
    "train_text_path='data/train_test/train.pkl'\n",
    "test_text_path='data/train_test/test.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text,test_text = importTextData()\n",
    "vocabs, glove_mat, tf_idf_vocabs, tf_idf_mat = importProcessedData()\n",
    "trainX,trainY,testX,testY = train_text.data, train_text.label, test_text.data, test_text.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3837</th>\n",
       "      <td>[[kerry, marks, opening, us, embassy, havana, ...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3256</th>\n",
       "      <td>[[clinton, struggles, contain, media, barrage,...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2029</th>\n",
       "      <td>[[revealed, several, ku, klux, klan, units, ac...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   data label\n",
       "3837  [[kerry, marks, opening, us, embassy, havana, ...  REAL\n",
       "3256  [[clinton, struggles, contain, media, barrage,...  REAL\n",
       "2029  [[revealed, several, ku, klux, klan, units, ac...  FAKE"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict, 54648)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(vocabs), len(vocabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, (54648, 100))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(glove_mat), glove_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict, 54648)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tf_idf_vocabs), len(tf_idf_vocabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, (6335, 200))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tf_idf_mat), tf_idf_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf idf 벡터 train, test로 분리\n",
    "train_tf_idf_mat=tf_idf_mat[train_text.index]\n",
    "test_tf_idf_mat=tf_idf_mat[test_text.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# label 벡터 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(trainY.unique()) # label  FAKE , REAL -> 0, 1 변경\n",
    "trainY=le.transform(trainY.tolist()).reshape([-1,1])\n",
    "testY=le.transform(testY.tolist()).reshape([-1,1])\n",
    "\n",
    "oe = preprocessing.OneHotEncoder() # one hot encoding\n",
    "oe.fit(trainY)\n",
    "trainY=oe.transform(trainY).toarray()\n",
    "testY=oe.transform(testY).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, (4434, 2))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(trainY), trainY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainY[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature 벡터 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_dim=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentenceToMeanVect(datasets,vocab,embeddingMat):\n",
    "    # datasets -> multiple news\n",
    "    # one news format : [ [ word1, word2 ...    ], [ word1, word2 ... ] ]\n",
    "    global glove_dim\n",
    "    new_datasets=np.zeros([len(datasets),glove_dim])\n",
    "    for news_index,dataset in enumerate(datasets):\n",
    "        one_news=np.zeros([len(dataset),glove_dim])\n",
    "        for sentence_idx,words in enumerate(dataset):\n",
    "            words_index_list=[]\n",
    "            for w in words:\n",
    "                if w in vocab.keys():\n",
    "                    words_index_list.append(vocab[w])\n",
    "            if len(words_index_list)!=0:\n",
    "                one_news[sentence_idx]=embeddingMat[words_index_list].mean(axis=0)\n",
    "        new_datasets[news_index]=one_news.mean(axis=0)\n",
    "            \n",
    "    return new_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentenceToMeanVect using Glove\n",
      "glove vect concat tf_idf\n"
     ]
    }
   ],
   "source": [
    "# data ~ ndarray format\n",
    "print('sentenceToMeanVect using Glove')\n",
    "trainX_vect=sentenceToMeanVect(trainX.tolist(),vocabs, glove_mat)\n",
    "testX_vect=sentenceToMeanVect(testX.tolist(),vocabs, glove_mat)\n",
    "\n",
    "print('glove vect concat tf_idf')\n",
    "trainX_vect = np.concatenate( (trainX_vect, train_tf_idf_mat), axis=1)\n",
    "testX_vect = np.concatenate( (testX_vect, test_tf_idf_mat), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, (4434, 300))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(trainX_vect), trainX_vect.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.0232491 ,  0.04178675,  0.10682759, -0.0266368 ,  0.05431599,\n",
       "       -0.02640858, -0.11489148,  0.04868763, -0.01568645,  0.00445718,\n",
       "       -0.02645362, -0.03948718,  0.12083414,  0.00492541, -0.02978713,\n",
       "       -0.05722605,  0.03868211, -0.05752463, -0.19206976, -0.04230576,\n",
       "        0.11625884,  0.02834731,  0.06748906,  0.036395  , -0.00531573,\n",
       "       -0.04176581, -0.05797076, -0.16345916,  0.09490456, -0.01657301,\n",
       "       -0.0087927 ,  0.18741142, -0.00189893,  0.06651688, -0.02551905,\n",
       "        0.10524439,  0.05502425,  0.04827616, -0.10352762,  0.00886553,\n",
       "       -0.30248996, -0.17512941,  0.12447057, -0.00243119,  0.01236538,\n",
       "       -0.12427934,  0.0639932 , -0.11708108, -0.05097423, -0.30999676,\n",
       "        0.09288945, -0.03844827,  0.03593322,  0.42547182, -0.05252739,\n",
       "       -0.95824803, -0.04663363, -0.08924524,  0.68118263,  0.22637737,\n",
       "       -0.1981861 ,  0.21309017, -0.05186182, -0.13296328,  0.19974258,\n",
       "       -0.0051713 ,  0.0854251 ,  0.18398628,  0.04925374, -0.04685655,\n",
       "        0.03420563, -0.10287887, -0.12169623, -0.197104  ,  0.07105642,\n",
       "        0.05857682, -0.02599074,  0.03143148, -0.41672271,  0.02945714,\n",
       "        0.25530212, -0.03855578, -0.08007933,  0.03229628, -0.51981021,\n",
       "       -0.05743173,  0.02658432,  0.01429341, -0.08375546, -0.25737334,\n",
       "        0.01537953, -0.05411789, -0.09548986,  0.01194936, -0.26050415,\n",
       "        0.03939296, -0.06176397, -0.06319518,  0.20472909,  0.08919795,\n",
       "        0.        ,  0.        ,  0.        ,  0.17503887,  0.        ,\n",
       "        0.06553175,  0.15764179,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.08431897,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.07352706,  0.        ,  0.        ,\n",
       "        0.0744828 ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.09259032,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.07075495,  0.        ,  0.1823265 ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.1864351 ,  0.        ,  0.        ,\n",
       "        0.        ,  0.07725388,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.17184294,  0.08956719,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.0985812 ,\n",
       "        0.        ,  0.        ,  0.        ,  0.17610032,  0.        ,\n",
       "        0.        ,  0.07147978,  0.        ,  0.        ,  0.22685967,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.07971938,\n",
       "        0.        ,  0.06693776,  0.08062029,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.05439823,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.06116213,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.08536832,  0.        ,  0.        ,\n",
       "        0.        ,  0.20081802,  0.06480058,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.3270559 ,  0.        ,  0.17067896,\n",
       "        0.        ,  0.        ,  0.        ,  0.14971137,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.05639395,\n",
       "        0.06514161,  0.16880545,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.08071929,  0.        ,  0.        ,  0.08379376,\n",
       "        0.06635828,  0.        ,  0.        ,  0.09408712,  0.22473407,\n",
       "        0.        ,  0.        ,  0.        ,  0.32618296,  0.        ,\n",
       "        0.06506715,  0.07661848,  0.        ,  0.16747799,  0.08330437,\n",
       "        0.        ,  0.        ,  0.        ,  0.19020156,  0.17129043,\n",
       "        0.06375689,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.13164007,  0.05388797,  0.0770993 ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.0718673 ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.06964701,  0.        ,  0.22914259,  0.06292738,\n",
       "        0.        ,  0.        ,  0.08580418,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.06162389,  0.16516925])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX_vect[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
